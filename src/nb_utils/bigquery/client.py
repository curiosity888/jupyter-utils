from google.cloud import bigquery
from google.cloud import bigquery_storage
from tqdm.notebook import tqdm
import pandas as pd
from nb_utils.options import config

def run_query(query):
    cfg = config.bigquery
    client = bigquery.Client(project=cfg.project_id)
    
    # --- DRY RUN ---
    dry_cfg = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
    print("‚ñ∂ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
    dry_job = client.query(query, job_config=dry_cfg)

    scanned_gb = dry_job.total_bytes_processed / (1024**3)
    
    print(f"üìä This query will process {scanned_gb:.2f} GB when run.")
    
    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    if scanned_gb > cfg.max_bytes_billed_gb:
        ans = input(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—Å —Å–∫–∞–Ω–∏—Ä—É–µ—Ç {scanned_gb:.2f} GB (> {cfg.max_bytes_billed_gb} GB). –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/n): ").strip().lower()
        if ans != "y":
            print("üö´ –û—Ç–º–µ–Ω–µ–Ω–æ.")
            return None

    # --- –í–´–ü–û–õ–ù–ï–ù–ò–ï –ó–ê–ü–†–û–°–ê ---
    use_storage = False
    print("‚ñ∂ –í—ã–ø–æ–ª–Ω—è—é –∑–∞–ø—Ä–æ—Å...")
    job = client.query(query)
    row_iter = job.result()

    # --- –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï API ---
    destination = job.destination
    temp_table = client.get_table(destination)
    use_storage = temp_table.num_rows >= cfg.min_rows_for_storage_api
    if use_storage:
        print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É—é **Storage API** (–æ–∂–∏–¥–∞–µ—Ç—Å—è {temp_table.num_rows} —Å—Ç—Ä–æ–∫)")
    else:
        print(f"üì¶ –ò—Å–ø–æ–ª—å–∑—É—é **REST API** (–æ–∂–∏–¥–∞–µ—Ç—Å—è {temp_table.num_rows} —Å—Ç—Ä–æ–∫)")

    # --- REST API ---
    if not use_storage:
        df = row_iter.to_dataframe(create_bqstorage_client=False)
        print(f"‚úì –ì–æ—Ç–æ–≤–æ, —Å—Ç—Ä–æ–∫: {len(df)} (REST API)")
        return df

    # --- STORAGE API ---
    bqstorage_client = bigquery_storage.BigQueryReadClient()
    arrow_iter = row_iter.to_arrow_iterable(bqstorage_client=bqstorage_client)

    dfs = []
    total_rows = 0

    for batch in tqdm(arrow_iter, desc="Downloading", unit="chunk", dynamic_ncols=True, mininterval=0.2):
        df_chunk = batch.to_pandas()
        dfs.append(df_chunk)
        total_rows += len(df_chunk)

    df = pd.concat(dfs, ignore_index=True)
    print(f"‚úì –ì–æ—Ç–æ–≤–æ, —Å—Ç—Ä–æ–∫: {total_rows} (Storage API)")
    return df